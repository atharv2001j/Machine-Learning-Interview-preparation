{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9dfbf3e",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">Machine Learning</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa54be2d",
   "metadata": {},
   "source": [
    "<h2> Linear Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda158d",
   "metadata": {},
   "source": [
    "\n",
    "Linear regression is a basic and widely used machine learning method. It helps make predictions for things like sales, salaries, ages, and product prices—basically, any number you want to predict.\n",
    "\n",
    "\n",
    "**The Idea of a Straight Line**:\n",
    "\n",
    "Imagine a straight line on a graph. Linear regression looks at the relationship between two things: one you want to predict (like sales) and another thing that you think affects it (like advertising spending). It tries to find a line that best fits the data points. This line tells you how the predicted value changes when the other variable changes.\n",
    "\n",
    "\n",
    "**For Example**:\n",
    "\n",
    "- If you're trying to predict how much someone will spend on a product based on how much they earn, linear regression helps you find that relationship.\n",
    "- It doesn't have to be just one thing; you can use it when you have several factors affecting the prediction.\n",
    "\n",
    "\n",
    "**So, in a nutshell**, linear regression helps you draw a straight line that represents how one thing depends on another, and you can use that line to make predictions about the future. It's a fundamental tool in data analysis and machine learning.\n",
    "\n",
    "\n",
    "<img src='linear.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138f467",
   "metadata": {},
   "source": [
    "Mathematically, we can represent a linear regression as:\n",
    "    \n",
    "<h3>y= a0 + a1x + ε</h3>\n",
    "\n",
    "Here,\n",
    "\n",
    "Y= Dependent Variable (Target Variable)\n",
    "\n",
    "X= Independent Variable (predictor Variable)\n",
    "\n",
    "a0= intercept of the line (Gives an additional degree of freedom)\n",
    "\n",
    "a1 = Linear regression coefficient (scale factor to each input value).\n",
    "\n",
    "ε = random error\n",
    "\n",
    "The values for x and y variables are training datasets for Linear Regression model representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85784b",
   "metadata": {},
   "source": [
    "<h2> Types of Linear Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90058a83",
   "metadata": {},
   "source": [
    "Linear regression can be further divided into two types of the algorithm:\n",
    "\n",
    "**1. Simple Linear Regression:**\n",
    "If a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression.\n",
    "\n",
    "\n",
    "**2. Multiple Linear regression:**\n",
    "If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671ae93",
   "metadata": {},
   "source": [
    "<h2> Linear Regression Line </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d26cb",
   "metadata": {},
   "source": [
    "A linear line showing the relationship between the dependent and independent variables is called a regression line. A regression line can show two types of relationship:\n",
    "\n",
    "**1. Positive Linear Relationship:**\n",
    "    \n",
    "If the dependent variable increases on the Y-axis and independent variable increases on X-axis, then such a relationship is termed as a Positive linear relationship.\n",
    "\n",
    "<img src='positive.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bbd39d",
   "metadata": {},
   "source": [
    "**2. Negative Linear Relationship:**\n",
    "    \n",
    "If the dependent variable decreases on the Y-axis and independent variable increases on the X-axis, then such a relationship is called a negative linear relationship.\n",
    "\n",
    "<img src='negative.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f31dd",
   "metadata": {},
   "source": [
    "<h2> Finding the Best fit line</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d7899",
   "metadata": {},
   "source": [
    "When we work with linear regression, our main goal is to discover the best-fit line. This line should minimize the difference between the values we predict and the actual values.\n",
    "\n",
    "**Different Lines, Different Coefficients**:\n",
    "To find the best-fit line, we need to determine the right values for the coefficients (like a0 and a1) in the linear equation. These coefficients change how the line looks.\n",
    "\n",
    "\n",
    "**The Role of the Cost Function**:\n",
    "The cost function is crucial. It helps us figure out the best coefficients for our line. It's like a tool to optimize our line.\n",
    "\n",
    "\n",
    "**What the Cost Function Measures**:\n",
    "The cost function tells us how well our linear regression model is performing. It measures the difference between what our model predicts and the real values.\n",
    "\n",
    "\n",
    "**Hypothesis Function**:\n",
    "The mapping function that connects input and output in linear regression is also known as the Hypothesis function. It's like the \"guessing\" function that the model uses.\n",
    "\n",
    "\n",
    "**Mean Squared Error (MSE) as the Cost Function**:\n",
    "In linear regression, we often use the Mean Squared Error (MSE) as our cost function. It calculates the average of the squared differences between our predictions and the actual values.\n",
    "\n",
    "\n",
    "**MSE Calculation**:\n",
    "For our linear equation, MSE can be calculated like this:\n",
    "```\n",
    "MSE = 1/N * Σ(Yi - (a1xi + a0))^2\n",
    "```\n",
    "Where:\n",
    "- N is the total number of observations.\n",
    "- Yi is the actual value.\n",
    "- (a1xi + a0) is the predicted value.\n",
    "\n",
    "\n",
    "**Residuals**:\n",
    "Residuals are the differences between the actual values and our predicted values. If the data points are far from our regression line, the residuals are high, and so is the cost function. If the points are close to the line, residuals are small, and the cost function is also small.\n",
    "\n",
    "In a nutshell, linear regression aims to find the best line to make predictions, and the cost function helps us tweak the line's coefficients to minimize the difference between our predictions and actual values. This way, we create a model that fits our data as closely as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d7ea4b",
   "metadata": {},
   "source": [
    "<h2>Gradient Descent </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71fc1ce",
   "metadata": {},
   "source": [
    "\n",
    "**Gradient Descent for Minimizing MSE**:\n",
    "\n",
    "- Gradient descent is like a navigation system for our regression model. It helps us find the best coefficients (the numbers that define our line) to make our predictions as accurate as possible.\n",
    "\n",
    "\n",
    "**Updating Coefficients**:\n",
    "\n",
    "- In a regression model, we have coefficients that we want to fine-tune to minimize the MSE. These coefficients affect the position and slope of the line.\n",
    "- Gradient descent helps us update these coefficients to make the line fit our data better.\n",
    "\n",
    "\n",
    "**The Role of the Cost Function**:\n",
    "\n",
    "- The cost function, in this case, is MSE (Mean Squared Error), which tells us how far off our predictions are from the actual values.\n",
    "\n",
    "\n",
    "**Iterative Process**:\n",
    "\n",
    "- Gradient descent doesn't find the best coefficients all at once. It starts with random values for the coefficients and then repeatedly updates them to reach the minimum MSE (the lowest point on the cost function).\n",
    "- At each step, it uses the gradient (a mathematical concept) of the cost function to figure out which direction to adjust the coefficients.\n",
    "- It's like climbing down a hill to find the lowest point: you take steps in the direction of steepest descent, and as you get closer to the bottom, your steps become smaller until you reach the minimum.\n",
    "\n",
    "\n",
    "So, in simple terms, gradient descent is the method we use to make our regression model learn and find the best-fitting line by adjusting the coefficients in a way that minimizes the error between our predictions and actual values. It's like a step-by-step process of fine-tuning our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5daaa",
   "metadata": {},
   "source": [
    "<h2> Model Performance </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222ac5c",
   "metadata": {},
   "source": [
    "The Goodness of fit determines how the line of regression fits the set of observations. The process of finding the best model out of various models is called optimization. It can be achieved by below method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd924d",
   "metadata": {},
   "source": [
    "<h3> R-Method </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a436df",
   "metadata": {},
   "source": [
    "\n",
    "**What is R-squared (R²)?**:\n",
    "\n",
    "- R-squared is like a report card for a linear regression model. It tells us how well our model fits the data.\n",
    "- It measures the strength of the relationship between the independent (input) and dependent (output) variables and expresses it as a percentage.\n",
    "\n",
    "\n",
    "**Interpreting R-squared**:\n",
    "\n",
    "- R-squared values range from 0 to 100%.\n",
    "- The closer R-squared is to 100%, the better our model is at explaining the data. In other words, a high R-squared means there's little difference between the predictions and the actual values. This is a good sign.\n",
    "\n",
    "\n",
    "**Other Names for R-squared**:\n",
    "\n",
    "- R-squared is also known as the coefficient of determination. In multiple regression (where you have more than one independent variable), it's called the coefficient of multiple determination.\n",
    "\n",
    "\n",
    "**Calculating R-squared**:\n",
    "\n",
    "- You can calculate R-squared using a formula. For simple linear regression, the formula is:\n",
    "```\n",
    "R² = 1 - (SSR / SST)\n",
    "```\n",
    "- In this formula, \n",
    "\n",
    "    SSR is the sum of the squared differences between the predicted values and the mean of the dependent variable, \n",
    "\n",
    "    SST is the sum of the squared differences between the actual values and the mean of the dependent variable.\n",
    "\n",
    "\n",
    "**What R-squared Tells Us**:\n",
    "\n",
    "- R-squared is a way to evaluate how well your linear regression model fits the data. A high R-squared indicates a strong relationship, while a low R-squared suggests that the model isn't explaining the data well.\n",
    "\n",
    "\n",
    "So, in a nutshell, R-squared is like a grade that tells us how well our linear regression model is doing. The closer it is to 100%, the better the model is at explaining the data, and that's what we aim for when building regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4b03b",
   "metadata": {},
   "source": [
    "<h2> Assumptions of Linear Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05659714",
   "metadata": {},
   "source": [
    "\n",
    "**1. Linear Relationship**:\n",
    "   - Linear regression assumes that the relationship between the independent variables and the target variable is straight and direct. In other words, when you plot the data points, they should form a roughly straight line.\n",
    "\n",
    "\n",
    "**2. Small or No Multicollinearity**:\n",
    "   - Multicollinearity means that independent variables are highly correlated with each other. Linear regression assumes that there is little to no multicollinearity.\n",
    "   - When variables are strongly related to each other, it becomes challenging to understand which one is influencing the target variable.\n",
    "\n",
    "    \n",
    "**3. Homoscedasticity**:\n",
    "   - Homoscedasticity is a situation where the variation in the error term is roughly the same for all values of independent variables.\n",
    "   - In simpler terms, it means that the data points are spread out consistently across the scatter plot, without forming any clear patterns.\n",
    "\n",
    "    \n",
    "**4. Normal Distribution of Error Terms**:\n",
    "   - Linear regression assumes that the errors (the differences between predicted and actual values) follow a normal distribution.\n",
    "   - If these errors are not normally distributed, it can affect the reliability of the model's coefficients and predictions.\n",
    "\n",
    "    \n",
    "**5. No Autocorrelations**:\n",
    "   - Autocorrelation occurs when there is a relationship between the errors from one observation and the errors from previous observations.\n",
    "   - Linear regression assumes that there is no such correlation, as it can significantly reduce the model's accuracy.\n",
    "\n",
    "    \n",
    "In essence, these assumptions act as a checklist to ensure that linear regression models work well. They help maintain the reliability and accuracy of the model's results, making it a useful tool for making predictions and understanding relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc811d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
