{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b920a5f",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">Machine Learning</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348632bf",
   "metadata": {},
   "source": [
    "<h2> Regression Analysis in Machine Learning </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58c23e",
   "metadata": {},
   "source": [
    "\n",
    "**Regression Analysis:**\n",
    "\n",
    "    \n",
    "1. **Purpose:** A statistical method used to model the relationship between a dependent (target) variable and one or more independent (predictor) variables.\n",
    "2. **Objective:** Helps us understand how the value of the dependent variable changes in response to changes in independent variables while keeping other independent variables constant.\n",
    "3. **Prediction:** Primarily used for predicting continuous or real values, such as temperature, age, salary, price, and other numerical quantities.\n",
    "4. **Types:** There are various types of regression, including linear regression, multiple regression, logistic regression (for binary outcomes), and more, each suited to different data and objectives.\n",
    "\n",
    "    \n",
    "Regression analysis is a fundamental tool in statistics and data analysis, allowing us to quantify and model the relationships between variables and make predictions based on those relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f010fa",
   "metadata": {},
   "source": [
    "\n",
    "**Regression Analysis:**\n",
    "\n",
    "1. **Purpose:** Regression is a supervised learning technique used to model the relationship between a dependent (target) variable and one or more independent (predictor) variables. It is primarily used for prediction, forecasting, understanding time series data, and determining causal-effect relationships between variables.\n",
    "\n",
    "\n",
    "2. **Visualization:** Regression often involves plotting a line or curve that best fits the data points, allowing the model to make predictions. The line or curve minimizes the vertical distance between data points and the regression line, indicating the strength of the relationship.\n",
    "\n",
    "\n",
    "3. **Examples:** Regression can be applied to various real-world scenarios, such as predicting rain based on temperature and other factors, determining market trends, and predicting road accidents due to reckless driving.\n",
    "\n",
    "\n",
    "**Terminologies:**\n",
    "\n",
    "- **Dependent Variable:** Also known as the target variable, this is the main factor that regression aims to predict or understand.\n",
    "\n",
    "\n",
    "- **Independent Variable:** These are factors that influence the dependent variable or are used to predict its values, also known as predictors.\n",
    "\n",
    "\n",
    "- **Outliers:** Outliers are observations with exceptionally high or low values compared to other data points, and they should be handled carefully as they can affect the results.\n",
    "\n",
    "\n",
    "- **Multicollinearity:** This occurs when independent variables are highly correlated with each other, which can complicate the ranking of the most influential variable.\n",
    "\n",
    "\n",
    "- **Underfitting and Overfitting:** Underfitting occurs when an algorithm performs poorly on both the training and test data, while overfitting occurs when it works well on the training data but poorly on the test data.\n",
    "\n",
    "\n",
    "**Why Use Regression Analysis:**\n",
    "\n",
    "- Regression estimates the relationship between variables.\n",
    "- It helps identify trends in data.\n",
    "- It's useful for predicting continuous values.\n",
    "- It aids in understanding how variables affect each other.\n",
    "\n",
    "\n",
    "**Types of Regression:**\n",
    "\n",
    "There are various types of regression used in data science and machine learning, depending on the specific problem and data. They all aim to analyze the effect of independent variables on dependent variables. Some important types include:\n",
    "\n",
    "- Linear Regression\n",
    "- Multiple Regression\n",
    "- Polynomial Regression\n",
    "- Logistic Regression (for binary outcomes)\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- and more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f929ef5",
   "metadata": {},
   "source": [
    "<src img='types_LR'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004797d8",
   "metadata": {},
   "source": [
    "<img src=\"types_LR.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3016cc",
   "metadata": {},
   "source": [
    "<h2> 1. Linear Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240359b",
   "metadata": {},
   "source": [
    "\n",
    "**Linear Regression:**\n",
    "\n",
    "    \n",
    "1. **Purpose:** Linear regression is a statistical method used for predictive analysis. It is particularly useful for understanding and modeling the relationship between continuous variables.\n",
    "\n",
    "2. **Simplicity:** Linear regression is one of the simplest regression algorithms used in machine learning.\n",
    "\n",
    "3. **Problem Solving:** It is commonly employed to solve regression problems in machine learning, where the goal is to predict a continuous output based on input variables.\n",
    "\n",
    "4. **Linear Relationship:** Linear regression reveals the linear relationship between the independent variable (X-axis) and the dependent variable (Y-axis), hence its name.\n",
    "\n",
    "5. **Types:** There are two main types of linear regression. Simple linear regression involves a single input variable (x), while multiple linear regression involves more than one input variable.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a8d8a",
   "metadata": {},
   "source": [
    "<img src=\"LR_Example.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57c99a",
   "metadata": {},
   "source": [
    "The relationship between variables in the linear regression model can be explained using the below image. Here we are predicting the salary of an employee on the basis of the year of experience.\n",
    "\n",
    "Below is the mathematical equation for Linear regression:\n",
    "\n",
    "                        Y= aX+b  \n",
    "Here, Y = dependent variables (target variables)\n",
    "\n",
    "X= Independent variables (predictor variables)\n",
    "\n",
    "a and b are the linear coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de51d271",
   "metadata": {},
   "source": [
    "<h3>Some popular applications of linear regression are:</h3>\n",
    "\n",
    "1. Analyzing trends and sales estimates\n",
    "2. Salary forecasting\n",
    "3. Real estate prediction\n",
    "4. Arriving at ETAs in traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574336e",
   "metadata": {},
   "source": [
    "<h2> 2. Logistic Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d8934",
   "metadata": {},
   "source": [
    "\n",
    "1. **Supervised Learning Algorithm**: Logistic regression is a supervised machine learning algorithm.\n",
    "\n",
    "\n",
    "2. **Classification Problems**: It is commonly used to solve classification problems, where the dependent variable is in a binary or discrete format (e.g., 0 or 1).\n",
    "\n",
    "\n",
    "3. **Categorical Variables**: Logistic regression works with categorical variables, making it suitable for predicting outcomes like Yes or No, True or False, Spam or not spam.\n",
    "\n",
    "\n",
    "4. **Predictive Analysis Algorithm**: Logistic regression is used for predictive analysis. It predicts the probability of an event occurring.\n",
    "\n",
    "\n",
    "5. **Distinction from Linear Regression**: Logistic regression is different from linear regression, primarily in terms of how they are used. While linear regression is used for predicting continuous numerical values, logistic regression is tailored for classification tasks.\n",
    "\n",
    "\n",
    "6. **Sigmoid Function**: Logistic regression uses the sigmoid function (logistic function), which is a complex cost function. This function models the probability of an input belonging to a particular category. The sigmoid function is represented as:\n",
    "\n",
    "   ```\n",
    "   f(x) = 1 / (1 + e^(-x))\n",
    "   ```\n",
    "f(x)= Output between the 0 and 1 value.\n",
    "\n",
    "    x= input to the function\n",
    "\n",
    "    e= base of natural logarithm.\n",
    "\n",
    "In summary, logistic regression is a supervised learning algorithm used for classification problems, particularly when dealing with categorical and binary outcomes. It leverages the sigmoid function to model probabilities, making it a valuable tool for various predictive analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca46ef95",
   "metadata": {},
   "source": [
    "<img src=\"sigmoid.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095c8f5",
   "metadata": {},
   "source": [
    "When we provide the input values (data) to the function, it gives the S-curve as above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa9752",
   "metadata": {},
   "source": [
    "<h4>There are three types of logistic regression</h4>\n",
    "\n",
    "1. Binary (0/1, pass/fail)\n",
    "2. Multi (cats, dogs, lions)\n",
    "3. Ordinal (low, medium, high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb08b46",
   "metadata": {},
   "source": [
    "<h2>3. Polynomial Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a4b5e",
   "metadata": {},
   "source": [
    "\n",
    "1. **Modeling Non-Linear Data**: Polynomial Regression is a regression technique used to model datasets with non-linear relationships. It is particularly useful when a simple linear regression model is inadequate for capturing the underlying patterns in the data.\n",
    "\n",
    "\n",
    "2. **Similarity to Multiple Linear Regression**: Polynomial regression is conceptually similar to multiple linear regression in that it uses a linear model. However, it extends the linear model to fit non-linear data by incorporating polynomial features.\n",
    "\n",
    "\n",
    "3. **Transforming Features**: In Polynomial Regression, the original features (independent variables) are transformed into polynomial features of a given degree. This transformation means that you create new features by raising the original features to various powers (e.g., squaring them, cubing them, etc.).\n",
    "\n",
    "\n",
    "4. **Polynomial Equation**: The Polynomial Regression equation is derived from the linear regression equation. While a simple linear regression equation is Y = b0 + b1x, the polynomial regression equation extends it to include polynomial terms, making it look like \n",
    "    \n",
    "    Y = b0 + b1x + b2x^2 + b3x^3 + ... + bnx^n, \n",
    "    \n",
    "    where Y is the predicted or target output, b0, b1, ..., bn are the regression coefficients, and x represents the independent/input variable.\n",
    "\n",
    "\n",
    "5. **Linear Model**: Despite the inclusion of polynomial terms, the model remains linear in terms of the regression coefficients. The linearity refers to the coefficients b0, b1, ..., bn, not the relationship between the independent and dependent variables, which can be non-linear due to the polynomial terms.\n",
    "\n",
    "\n",
    "In summary, Polynomial Regression is a valuable technique in machine learning for modeling non-linear relationships. It allows you to transform original features into polynomial features and fit a linear model to capture the complexities in the data. The model's linearity relates to the coefficients while accommodating non-linear data patterns through the polynomial terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d140f0",
   "metadata": {},
   "source": [
    "<img src='polynomial.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4b88c",
   "metadata": {},
   "source": [
    "<h2> 4. Support Vector Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a08a2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Support Vector Machine (SVM)**: SVM is a versatile supervised learning algorithm that can be used for both classification and regression tasks. When applied to regression, it is referred to as Support Vector Regression (SVR).\n",
    "\n",
    "    \n",
    "2. **Continuous Variables**: SVR is used for regression problems where the target variable is continuous. It helps in predicting a continuous output, unlike Support Vector Classification (SVC), which predicts class labels.\n",
    "\n",
    "    \n",
    "3. **Keywords in SVR**:\n",
    "   - **Kernel**: Kernels are functions used to map data from a lower-dimensional space to a higher-dimensional space. This transformation can help SVR capture complex relationships between features and the target variable.\n",
    "   - **Hyperplane**: In classical SVM, a hyperplane is a separation line that distinguishes between two classes. In SVR, it represents a line that helps predict continuous variables and is aimed at capturing as many data points as possible within a certain margin.\n",
    "   - **Boundary Lines**: These are the lines on both sides of the hyperplane that create a margin for data points. In SVR, the goal is to maximize the number of data points within this margin.\n",
    "   - **Support Vectors**: Support vectors are data points that are closest to the hyperplane. They are critical for defining the margin and determining the position of the hyperplane.\n",
    "\n",
    "    \n",
    "4. **Objective of SVR**: In SVR, the primary objective is to find a hyperplane with a maximum margin while considering as many data points as possible within the margin. This means finding the best-fit line that contains the maximum number of data points within a specified margin.\n",
    "\n",
    "<img src='svr.png'>\n",
    "\n",
    "Here, the blue line is called hyperplane, and the other two lines are known as boundary lines.\n",
    "\n",
    "\n",
    "In summary, Support Vector Regression is a regression technique that employs the principles of SVM, focusing on finding a hyperplane with a maximum margin to predict continuous target variables. It utilizes concepts such as kernels, hyperplanes, boundary lines, and support vectors to achieve this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97d0c7",
   "metadata": {},
   "source": [
    "<h2> 5. Decision Tree Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01721eef",
   "metadata": {},
   "source": [
    "\n",
    "1. **Supervised Learning Algorithm for Classification and Regression**: Decision trees are a versatile supervised learning algorithm used for both classification and regression problems.\n",
    "\n",
    "\n",
    "2. **Handling Categorical and Numerical Data**: Decision trees can handle data with both categorical and numerical features, making them applicable to a wide range of data types.\n",
    "\n",
    "\n",
    "3. **Tree-Like Structure**: Decision tree regression indeed builds a tree-like structure where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents the final decision or result.\n",
    "\n",
    "\n",
    "4. **Construction Process**: Decision trees are constructed starting from a root node, which represents the entire dataset. This root node is split into left and right child nodes, corresponding to subsets of the dataset. These child nodes can further divide into their children, and the process continues recursively, with each internal node representing a test or decision point.\n",
    "\n",
    "<img src=\"decision.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163f913",
   "metadata": {},
   "source": [
    "Above image showing the example of Decision Tee regression, here, the model is trying to predict the choice of a person between Sports cars or Luxury car."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b22ee0",
   "metadata": {},
   "source": [
    "<h4> Random Forest Algorithm </h4>\n",
    "1. **Random Forest for Regression and Classification**:\n",
    "   - Random Forest is indeed a powerful supervised learning algorithm capable of performing both regression and classification tasks. It is a versatile tool for various machine learning problems.\n",
    "\n",
    "\n",
    "2. **Random Forest Regression and Ensemble Learning**:\n",
    "   - Random Forest regression is an ensemble learning method that combines multiple decision trees to make predictions. It predicts the final output based on the average (or mode in classification) of the outputs from individual decision trees.\n",
    "   - The individual decision trees within a Random Forest are often referred to as \"base models\" or \"weak learners.\"\n",
    "\n",
    "\n",
    "3. **Formal Representation**:\n",
    "   - The combination of decision trees in a Random Forest can be formally represented as:\n",
    "     g(x) = f0(x) + f1(x) + f2(x) + ...\n",
    "   - Each f(x) represents the output of an individual decision tree.\n",
    "\n",
    "\n",
    "4. **Bagging (Bootstrap Aggregation)**:\n",
    "   - Random Forest uses the Bagging technique (Bootstrap Aggregation) as part of its ensemble learning approach. Bagging involves creating multiple subsets of the dataset through random sampling with replacement (bootstrap samples).\n",
    "   - Each decision tree in the Random Forest is trained on a different subset of the data.\n",
    "   - These decision trees run in parallel and do not interact with each other during the learning process. They are built independently.\n",
    "\n",
    "\n",
    "5. **Preventing Overfitting**:\n",
    "   - Random Forest is effective at preventing overfitting because of the randomness introduced through bootstrapped subsets and feature selection at each node of the decision trees.\n",
    "   - By training each decision tree on a different subset of the data, Random Forest reduces the risk of overfitting the model to the training data.\n",
    "\n",
    "\n",
    "Random Forest is indeed a robust and popular ensemble learning algorithm known for its ability to handle a variety of tasks, reduce overfitting, and provide reliable predictions through the combination of multiple decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ca567",
   "metadata": {},
   "source": [
    "<img src=\"random.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54767e",
   "metadata": {},
   "source": [
    "<h2> 6. Ridge Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ceef36",
   "metadata": {},
   "source": [
    "\n",
    "1. **Introduction to Ridge Regression**:\n",
    "   - Ridge Regression is a type of linear regression used in machine learning.\n",
    "   - It's a robust version of regular linear regression, which helps us make better predictions over the long term.\n",
    "\n",
    "\n",
    "2. **Ridge Regression Penalty**:\n",
    "   - In Ridge Regression, a small amount of bias is introduced into the model.\n",
    "   - This bias is referred to as the \"Ridge Regression penalty.\"\n",
    "   - To calculate this penalty, we take a value called \"lambda\" (λ) and multiply it by the squared weight of each feature.\n",
    "\n",
    "\n",
    "3. **When to Use Ridge Regression**:\n",
    "   - Ridge Regression is useful when you have independent variables (features) that are highly correlated, meaning they move together in a predictable way.\n",
    "   - In a regular linear or polynomial regression, having high collinearity between features can lead to unreliable predictions.\n",
    "\n",
    "\n",
    "4. **Complexity Reduction**:\n",
    "   - Ridge Regression is a regularization technique, specifically called L2 regularization.\n",
    "   - Its primary goal is to reduce the complexity of the regression model.\n",
    "   - This means it helps prevent the model from becoming too sensitive to small variations in the data, which can lead to overfitting (making predictions based on noise in the data).\n",
    "\n",
    "\n",
    "5. **More Parameters than Samples**:\n",
    "   - Ridge Regression comes to the rescue when you have more parameters (features) to consider than you have data samples.\n",
    "   - In such cases, traditional linear regression can struggle because it might try to fit the data perfectly, even when there isn't enough data to support it. Ridge Regression helps by introducing that necessary bias and preventing overfitting.\n",
    "\n",
    "\n",
    "In a nutshell, Ridge Regression is like a smart tool in machine learning that helps when you have lots of correlated features, maintains long-term prediction accuracy by introducing a little bias, and avoids overcomplicating the model when you don't have enough data. It's a useful method for building more reliable models in such scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e687c",
   "metadata": {},
   "source": [
    "Y = β0 + β1*X1 + β2*X2 + ... + βn*Xn + λ*(β1^2 + β2^2 + ... + βn^2)\n",
    "\n",
    "- Y is the predicted output (dependent variable).\n",
    "\n",
    "- X1, X2, ..., Xn are the independent variables (features).\n",
    "\n",
    "- β0 is the intercept (the value of Y when all Xs are zero).\n",
    "\n",
    "- β1, β2, ..., βn are the coefficients for each independent variable.\n",
    "\n",
    "- λ (lambda) is the regularization parameter, controlling the amount of Ridge penalty.\n",
    "\n",
    "- (β1^2 + β2^2 + ... + βn^2) represents the sum of the squared values of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9fdbb",
   "metadata": {},
   "source": [
    "<h2> 7. Lasso Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05451ab",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Lasso Regression**:\n",
    "\n",
    "Lasso Regression, short for \"Least Absolute Shrinkage and Selection Operator,\" is a regularization technique used in machine learning. It's similar to Ridge Regression in that it's used to reduce the complexity of a model, but it has some key differences.\n",
    "\n",
    "\n",
    "**The Lasso Regression Penalty Term**:\n",
    "\n",
    "The primary distinction between Lasso and Ridge Regression lies in the penalty term. In Lasso Regression, the penalty term contains the absolute values of the weights (coefficients) instead of their squares, as in Ridge Regression.\n",
    "\n",
    "\n",
    "**Shrinking Coefficients to Zero**:\n",
    "\n",
    "Lasso Regression's unique feature is that it can shrink the coefficients all the way to zero. In contrast, Ridge Regression can only shrink the coefficients near zero but not exactly to zero. This makes Lasso useful for feature selection because it effectively sets some coefficients to zero, indicating that those features have no impact on the model's predictions.\n",
    "\n",
    "\n",
    "**L1 Regularization**:\n",
    "\n",
    "Lasso Regression is often referred to as L1 regularization, indicating that it uses the absolute values of coefficients as a penalty term. This has the effect of encouraging sparsity in the model, meaning it promotes a simpler model with fewer features, as some coefficients are set to zero.\n",
    "\n",
    "\n",
    "**Lasso Regression Equation**:\n",
    "\n",
    "The equation for Lasso Regression can be represented as follows:\n",
    "\n",
    "\n",
    "```\n",
    "Y = β0 + β1*X1 + β2*X2 + ... + βn*Xn + λ*(|β1| + |β2| + ... + |βn|)\n",
    "```\n",
    "\n",
    "- **Y** is the predicted output (dependent variable).\n",
    "- **X1, X2, ..., Xn** are the independent variables (features).\n",
    "- **β0** is the intercept (the value of Y when all Xs are zero).\n",
    "- **β1, β2, ..., βn** are the coefficients for each independent variable.\n",
    "- **λ (lambda)** is the regularization parameter, controlling the amount of Lasso penalty.\n",
    "- **(|β1| + |β2| + ... + |βn|)** represents the sum of the absolute values of the coefficients.\n",
    "\n",
    "In Lasso Regression, the absolute values of the coefficients are added to the model's cost function. As a result, Lasso encourages a sparse model with only the most important features, as it can set some coefficient values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87690be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
