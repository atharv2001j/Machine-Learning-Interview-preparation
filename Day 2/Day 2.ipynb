{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff88f8f",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">Machine Learning</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043086d",
   "metadata": {},
   "source": [
    "<h2> Installing Anaconda and Python : </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961baab6",
   "metadata": {},
   "source": [
    "\n",
    "1. **Download Anaconda**:\n",
    "   - Open your web browser and go to the Anaconda download page.\n",
    "   - Choose the appropriate version (Windows, Linux, or macOS).\n",
    "   - Download the latest Python 3.7 version.\n",
    "\n",
    "\n",
    "2. **Install Anaconda**:\n",
    "   - Run the downloaded Anaconda installer (e.g., Anaconda3-2019.03-Windows-x86_64.exe).\n",
    "   - Follow the installation wizard.\n",
    "     - Agree to the license agreement.\n",
    "     - Select the installation options (e.g., \"Just me\" for individual use).\n",
    "     - Choose the installation location (you can leave it as the default).\n",
    "     - Complete the installation.\n",
    "\n",
    "\n",
    "3. **Open Anaconda Navigator**:\n",
    "   - Use the Windows Start menu to search for \"Anaconda Navigator.\"\n",
    "   - Launch Anaconda Navigator.\n",
    "\n",
    "\n",
    "4. **Launch Spyder IDE**:\n",
    "   - In Anaconda Navigator, click on the \"Launch\" button next to Spyder.\n",
    "   - This will install the Spyder IDE on your system.\n",
    "\n",
    "\n",
    "5. **Run Python Programs in Spyder**:\n",
    "   - Open Spyder IDE, and it will provide a Python programming environment.\n",
    "   - Write your Python programs in Spyder.\n",
    "   - Save your program with a .py extension.\n",
    "   - Run the program using the \"Run\" button.\n",
    "   - Check the program's output in the console pane at the bottom right.\n",
    "\n",
    "\n",
    "6. **Close Spyder IDE**:\n",
    "   - When you're done, you can close the Spyder IDE.\n",
    "\n",
    "\n",
    "This installation process provides you with Anaconda, which includes Python and various IDEs (including Spyder) for running and developing Python programs, making it a convenient solution for machine learning and data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1759f4",
   "metadata": {},
   "source": [
    "<h2> Artificial Intelligence Vs Machine Learning : </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af43f7",
   "metadata": {},
   "source": [
    "\n",
    "**Artificial Intelligence (AI):**\n",
    "- AI is a technology that enables machines to simulate human behavior.\n",
    "- The goal of AI is to create smart computer systems that can solve complex problems, similar to humans.\n",
    "- AI aims to make intelligent systems that can perform various complex tasks.\n",
    "- It has a wide scope and includes a broad range of applications, such as Siri, customer support chatbots, expert systems, and intelligent humanoid robots.\n",
    "- AI can be categorized into Weak AI, General AI, and Strong AI based on capabilities.\n",
    "- AI involves learning, reasoning, and self-correction.\n",
    "- AI deals with structured, semi-structured, and unstructured data.\n",
    "\n",
    "**Machine Learning (ML):**\n",
    "- ML is a subset of AI that allows machines to automatically learn from past data without explicit programming.\n",
    "- The goal of ML is to enable machines to learn from data and provide accurate outputs.\n",
    "- ML teaches machines to perform specific tasks for which they are trained.\n",
    "- It has a limited scope compared to AI and mainly focuses on specific task-based applications.\n",
    "- ML includes subcategories like supervised learning, unsupervised learning, and reinforcement learning.\n",
    "- ML involves learning and self-correction when introduced with new data.\n",
    "- ML primarily deals with structured and semi-structured data.\n",
    "\n",
    "In summary, AI is a broader concept focused on creating intelligent systems that can perform various complex tasks, while ML is a subset of AI that specifically deals with teaching machines to learn from data and perform specific tasks accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f89d3b5",
   "metadata": {},
   "source": [
    "<h2>How to get datasets for Machine Learning : </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b8ecb",
   "metadata": {},
   "source": [
    "\n",
    "**What is a Dataset:**\n",
    "- A dataset is a collection of data arranged in some order.\n",
    "- It can be represented as a table with rows and columns, where each column corresponds to a variable, and each row represents data points.\n",
    "- Common file formats for tabular datasets include CSV, while JSON is suitable for tree-like data.\n",
    "\n",
    "**Types of Data in Datasets:**\n",
    "- Numerical data includes values like house prices or temperatures.\n",
    "- Categorical data includes categories like Yes/No, True/False, or colors.\n",
    "- Ordinal data is similar to categorical data but can be measured based on comparisons.\n",
    "\n",
    "**Types of Datasets:**\n",
    "1. **Image Datasets:**\n",
    "   - Contain various images, used in computer vision tasks such as image classification and object detection.\n",
    "   - Examples: ImageNet, CIFAR-10, MNIST.\n",
    "\n",
    "2. **Text Datasets:**\n",
    "   - Comprise textual information like articles or movie reviews, used in natural language processing (NLP) tasks.\n",
    "   - Examples: Gutenberg Task dataset, IMDb movie reviews dataset.\n",
    "\n",
    "3. **Time Series Datasets:**\n",
    "   - Involve data points collected over time, used in forecasting and trend analysis.\n",
    "   - Examples: Stock market data, weather data, sensor readings.\n",
    "\n",
    "4. **Tabular Datasets:**\n",
    "   - Organized in tables, suitable for tasks like regression and classification.\n",
    "   - Example: The provided sample dataset in the article.\n",
    "\n",
    "**Need of Dataset:**\n",
    "- Well-prepared datasets are crucial for machine learning projects.\n",
    "- They serve as the foundation for training accurate and reliable models.\n",
    "- However, working with large datasets can be challenging, requiring efficient data management techniques and algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810a0ca",
   "metadata": {},
   "source": [
    "<h3> Data Preprocessing </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0cd14",
   "metadata": {},
   "source": [
    "\n",
    "**Data Pre-processing:**\n",
    "- Data pre-processing is a crucial stage in preparing datasets for machine learning.\n",
    "- It involves transforming raw data into a suitable format for model training.\n",
    "- Common pre-processing techniques include data cleaning, standardization, feature scaling, and handling missing values.\n",
    "\n",
    "**Training Dataset and Test Dataset:**\n",
    "- In machine learning, datasets are typically divided into two parts:\n",
    "  1. **Training Dataset:** Used to train the machine learning model.\n",
    "  2. **Test Dataset:** Used to evaluate the model's performance.\n",
    "- The division ensures the model's ability to generalize to new, unseen data.\n",
    "- Datasets should be representative of the problem and properly split to avoid bias or overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57042c7b",
   "metadata": {},
   "source": [
    "<h3> Popular sources for Machine Learning datasets : </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4bb50",
   "metadata": {},
   "source": [
    "These are various sources for obtaining datasets for machine learning:\n",
    "\n",
    "1. **Kaggle Datasets:**\n",
    "   - Kaggle is a popular platform for data scientists and machine learners.\n",
    "   - It offers a wide range of high-quality datasets in different formats.\n",
    "   - You can find, download, and collaborate with others on data science-related projects.\n",
    "   - [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "\n",
    "2. **UCI Machine Learning Repository:**\n",
    "   - An important resource used by researchers and specialists since 1987.\n",
    "   - Contains a vast collection of datasets categorized by machine learning tasks like regression, classification, and clustering.\n",
    "   - Notable datasets include Iris, Vehicle Assessment, and Poker Hand.\n",
    "   - [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
    "\n",
    "3. **Datasets via AWS:**\n",
    "   - Access datasets available through AWS resources, provided by various organizations and individuals.\n",
    "   - These datasets can be accessed for analysis, reducing the time spent on data acquisition.\n",
    "   - [Registry of Open Data on AWS](https://registry.opendata.aws/)\n",
    "\n",
    "4. **Google's Dataset Search Engine:**\n",
    "   - Google's Dataset Search helps researchers find and access datasets from various sources on the web.\n",
    "   - It covers areas like social sciences, science, and environmental science.\n",
    "   - Users can search for datasets using keywords and access them directly from the source.\n",
    "   - [Google's Dataset Search Engine](https://toolbox.google.com/datasetsearch)\n",
    "\n",
    "5. **Microsoft Datasets:**\n",
    "   - Microsoft Research Open Data offers free datasets in areas like natural language processing, computer vision, and domain-specific sciences.\n",
    "   - Access diverse and organized datasets that can be valuable for machine learning projects.\n",
    "   - [Microsoft Research Open Data](https://msropendata.com/)\n",
    "\n",
    "6. **Awesome Public Dataset Collection:**\n",
    "   - Provides high-quality datasets organized by topics such as Agriculture, Biology, and Climate.\n",
    "   - Most datasets are free, but it's essential to check the license before downloading.\n",
    "   - [Awesome Public Dataset Collection](https://github.com/awesomedata/awesome-public-datasets)\n",
    "\n",
    "7. **Government Datasets:**\n",
    "   - Governments from various countries publish data collected from different departments for public use.\n",
    "   - The goal is to increase transparency and encourage innovative use of government data.\n",
    "   - Examples include:\n",
    "     - [Indian Government dataset](https://data.gov.in/)\n",
    "     - [US Government Dataset](https://www.data.gov/)\n",
    "     - [European Union Open Data Portal](https://data.europa.eu/euodp/en/home)\n",
    "     \n",
    "8. **Computer Vision Datasets:**\n",
    "   - Specifically for computer vision tasks like image classification, video classification, and image segmentation.\n",
    "   - Ideal for projects in deep learning or image processing.\n",
    "   - [Visual Data](https://www.visualdata.io/)\n",
    "\n",
    "9. **Scikit-learn Dataset:**\n",
    "   - Scikit-learn, a popular machine learning library in Python, offers several built-in datasets for practice and experimentation.\n",
    "   - These datasets can be accessed through the Scikit-learn API and are useful for learning various machine learning algorithms.\n",
    "   - Examples include the Iris dataset, Boston Housing dataset, and Wine dataset.\n",
    "   - [Scikit-learn Datasets](https://scikit-learn.org/stable/datasets/index.html)\n",
    "\n",
    "These sources provide a wealth of datasets for various machine learning applications and research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5048870",
   "metadata": {},
   "source": [
    "<h1> Data Pre-processing : </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a7cec",
   "metadata": {},
   "source": [
    "Data preprocessing is indeed a critical step in preparing data for machine learning models. It involves various tasks to clean and format the data so that it can be used effectively. The steps you've mentioned are essential for data preprocessing, and I'll provide a brief summary of each:\n",
    "\n",
    "1. **Getting the Dataset:**\n",
    "   - Collect or obtain the dataset that you'll use for your machine learning project. Datasets can be in various formats, such as CSV, HTML, or Excel.\n",
    "   \n",
    "\n",
    "2. **Importing Libraries:**\n",
    "   - Import necessary libraries for data manipulation, analysis, and visualization.\n",
    "   - Common libraries include NumPy, Matplotlib, and Pandas.\n",
    "\n",
    "\n",
    "3. **Importing the Dataset:**\n",
    "   - Load your dataset into your Python environment using functions like `pd.read_csv()` for CSV files.\n",
    "   - Ensure that your Python script is in the same directory as your dataset or specify the file path correctly.\n",
    "\n",
    "\n",
    "4. **Handling Missing Data:**\n",
    "   - Identify and handle missing values in your dataset.\n",
    "   - Common methods include removing rows or columns with missing data or imputing missing values using the mean, median, or mode of the respective feature.\n",
    "\n",
    "\n",
    "5. **Encoding Categorical Data:**\n",
    "   - Convert categorical variables (text data) into a numerical format since many machine learning algorithms require numeric input.\n",
    "   - Techniques like Label Encoding (assigning unique numbers to categories) and One-Hot Encoding (creating binary columns for each category) can be used.\n",
    "\n",
    "\n",
    "6. **Splitting the Dataset into Training and Test Sets:**\n",
    "   - Divide your dataset into two subsets: a training set and a test set.\n",
    "   - The training set is used to train your machine learning model, while the test set is used to evaluate the model's performance.\n",
    "   - Typically, this is done using functions like `train_test_split` from libraries like scikit-learn.\n",
    "\n",
    "\n",
    "7. **Feature Scaling:**\n",
    "   - Scale your feature variables to ensure they are on a similar scale.\n",
    "   - Common methods include Standardization (scaling features to have mean 0 and variance 1) and Normalization (scaling features to a specific range, often [0, 1]).\n",
    "\n",
    "\n",
    "These are the fundamental steps of data preprocessing for machine learning. Depending on your dataset and the specific machine learning model you're working with, you may need to perform additional data preprocessing steps, such as feature engineering or dimensionality reduction. Data preprocessing is a critical part of the machine learning pipeline, as the quality of your data directly impacts the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32882eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries  \n",
    "import numpy as nm  \n",
    "import matplotlib.pyplot as mtp  \n",
    "import pandas as pd  \n",
    "  \n",
    "#importing datasets  \n",
    "data_set= pd.read_csv('Dataset.csv')  \n",
    "  \n",
    "#Extracting Independent Variable  \n",
    "x= data_set.iloc[:, :-1].values  \n",
    "  \n",
    "#Extracting Dependent variable  \n",
    "y= data_set.iloc[:, 3].values  \n",
    "  \n",
    "#handling missing data(Replacing missing data with the mean value)  \n",
    "from sklearn.preprocessing import Imputer  \n",
    "imputer= Imputer(missing_values ='NaN', strategy='mean', axis = 0)  \n",
    "  \n",
    "#Fitting imputer object to the independent varibles x.   \n",
    "imputerimputer= imputer.fit(x[:, 1:3])  \n",
    "  \n",
    "#Replacing missing data with the calculated mean value  \n",
    "x[:, 1:3]= imputer.transform(x[:, 1:3])  \n",
    "  \n",
    "#for Country Variable  \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  \n",
    "label_encoder_x= LabelEncoder()  \n",
    "x[:, 0]= label_encoder_x.fit_transform(x[:, 0])  \n",
    "  \n",
    "#Encoding for dummy variables  \n",
    "onehot_encoder= OneHotEncoder(categorical_features= [0])    \n",
    "x= onehot_encoder.fit_transform(x).toarray()  \n",
    "  \n",
    "#encoding for purchased variable  \n",
    "labelencoder_y= LabelEncoder()  \n",
    "y= labelencoder_y.fit_transform(y)  \n",
    "  \n",
    "# Splitting the dataset into training and test set.  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state=0)  \n",
    "  \n",
    "#Feature Scaling of datasets  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "st_x= StandardScaler()  \n",
    "x_train= st_x.fit_transform(x_train)  \n",
    "x_test= st_x.transform(x_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
